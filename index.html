<!DOCTYPE HTML>
<html lang="en"><head>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Miguel Farinha</title>
  
  <meta name="author" content="Miguel Farinha">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style type="text/css">
    /* fonts omitted for brevity — keep as is */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .fade {
      transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }

    .phone-teasers {
      display: none;
    }

    @media only screen and (max-width: 768px) {
      .abstract {
        display: none;
      }

      .comp-teasers {
        display: none;
      }

      .phone-teasers {
        display: block;
      }
    }

    /* --- Modal styles for image expansion --- */
    .modal {
      display: none;
      position: fixed;
      z-index: 1000;
      padding-top: 60px;
      left: 0;
      top: 0;
      width: 100%;
      height: 100%;
      overflow: auto;
      background-color: rgba(0,0,0,0.9);
    }

    .modal-content {
      margin: auto;
      display: block;
      max-width: 90%;
      max-height: 80vh;
      border-radius: 6px;
    }

    .close {
      position: absolute;
      top: 20px;
      right: 35px;
      color: #fff;
      font-size: 30px;
      font-weight: bold;
      cursor: pointer;
    }
  </style>

  <link rel="icon" class="top-icon" type="image/png" href="images/icon.png">

  <script>
    const icons = [
      "images/icons/sunglasses.png",
      "images/icons/computer.png",
      "images/icons/cowboy.png",
      "images/icons/smile.png"
    ];
    const randomIcon = icons[Math.floor(Math.random() * icons.length)];
    const iconElement = document.querySelector(".top-icon");
    iconElement.href = randomIcon;
  </script>

</head>

<body>
  <table style="width:100%;max-width:900px;border:0;margin:auto;"><tbody>
    <tr>
      <td>
        <table style="width:100%;border:0;margin:auto;"><tbody>
          <tr>
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Miguel Farinha</name>
              </p>
              <p>Hello! I am a CS PhD student at the University of Oxford working with <a target="_blank" href="https://ronnie-clark.co.uk/index.html">Professor Ronald Clark</a> on building <strong>foundation models for 3D understanding and perception</strong>.</p> 
              
              <p>I obtained my masters degree at the University of Lisbon studying Applied Mathematics with a focus on Probability and Statistics and my bachelors degree at the University of Lisbon studying Biomedical Engineering.</p>
              <p style="text-align:center">
                    [
                <a target="_blank" href="mailto:miguelffarinha@gmail.com">Email</a> &nbsp;/&nbsp;
                <a target="_blank" href="https://github.com/mlfarinha/">Github</a> &nbsp;/&nbsp;
                <a target="_blank" href="https://scholar.google.com/citations?user=HE6se3sAAAAJ&hl=pt-PT">Google Scholar</a> &nbsp;/&nbsp;
                <a target="_blank" href="https://www.linkedin.com/in/miguel-farinha-973a051b5/">LinkedIn</a> &nbsp;/&nbsp;
                <a target="_blank" href="images/cv.pdf">CV</a>
                    ]
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a target="_blank"><img style="width:100%;max-width:100%" alt="profile photo" src="images/miguel.jpeg"></a>
            </td>
          </tr>
        </tbody></table>

        <h2>Works in Progress</h2>
        <table style="width:100%;border:0;margin:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle" class="comp-teasers">
              <img src="images/sd3_recon.jpg" alt="hpp" style="border-style:none;width:350px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Estimating Geometry and Pose for Scene Reconstruction</papertitle>
              <p class="summary">Unifying depth and camera motion estimation from RGB images to enable consistent 3D scene reconstruction.</p>
              
              <a href="javascript:void(0)" class="toggle-overview">[Overview]</a>
              <div class="abstract" style="display:none;">
                This project develops a framework that jointly estimates scene geometry (depth) 
                and camera motion directly from pairs of RGB images. We combine <strong>visual 
                foundation models</strong> (e.g., SD3.5, DINOv2) with <strong>geometric optimization</strong> 
                to refine predictions into a consistent 3D representation.  
                <br><br>
                Our approach performs gradient descent over a simple least-squares objective, 
                aligning predicted optical flow with correspondences from an off-the-shelf 
                flow estimator. By fine-tuning pretrained foundation models to predict 
                affine-invariant depth and optical flow jointly, we bridge geometry and motion 
                estimation in a single unified system.  
                <br><br>
                <strong>Contribution:</strong> A method that integrates learning-based priors with 
                optimization, enabling robust 3D scene reconstruction from multiple RGB views.
              </div>
              <div class="phone-teasers" style="display:flex;justify-content:center;margin-top:1em;">
                <img class="phone-teasers" src="images/sd3_recon.jpg" alt="hpp" style="border-style:none;width:300px">
              </div>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle" class="comp-teasers">
              <img src="images/svd_relighting.jpg" alt="hpp" style="border-style:none;width:350px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Scene Understanding and Perception using Multi-view Diffusion Priors</papertitle>
              <p class="summary">Building complete 3D scene models, including geometry and material properties, from multi-view RGB images.</p>
              
              <a href="javascript:void(0)" class="toggle-overview">[Overview]</a>
              <div class="abstract" style="display:none;">
                This work aims to recover a full 3D representation of indoor scenes — geometry 
                (depth and normals) and appearance (SVBRDF) — directly from RGB images.  
                <br><br>
                We adapt the <strong>Stable Video Diffusion</strong> model to a multi-view setting, 
                conditioning it on several images of a scene and training it to output depth 
                maps, surface normals, and spatially varying reflectance properties. This 
                approach leverages <strong>diffusion priors</strong> for learning consistent multi-view 
                geometry and appearance estimation.  
                <br><br>
                <strong>Contribution:</strong> A novel diffusion-based method that jointly estimates 
                geometry and SVBRDFs, providing a complete 3D scene model. This enables 
                downstream applications such as relighting, novel view synthesis, and 3D 
                reconstruction.
              </div>
              <div class="phone-teasers" style="display:flex;justify-content:center;margin-top:1em;">
                <img class="phone-teasers" src="images/svd_relighting.jpg" alt="hpp" style="border-style:none;width:300px">
              </div>
            </td>
          </tr>
        </tbody></table>

        <h2>Past Works</h2>
        <table style="width:100%;border:0;margin:auto;"><tbody>
  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle" class="comp-teasers">
              <img src="images/clouds.jpg" alt="hpp" style="border-style:none;width:350px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Volumetric Cloud Field Reconstruction</papertitle>
              <div>
                <span style="font-size:13px;">
                  Jacob Lin, <strong>Miguel Farinha</strong>, Edward Gryspeerdt, Ronald Clark
                </span>
              </div>
              <div>
                <em>arxiv, 2023</em>
              </div>
              <div>
                [
                  <a target="_blank" href="https://arxiv.org/abs/2311.17657">Paper</a>  &nbsp;/&nbsp;
                  <a target="_blank" href="https://cloud-field.github.io">Project Page</a>
                ]
              </div>
              <p class="summary">
                Reconstructing cloud density and motion fields from sparse stereo pairs using deep learning.
              </p>
              
              <a href="javascript:void(0)" class="toggle-overview">[Overview]</a>
              <div class="abstract" style="display:none;">
                Volumetric phenomena such as clouds are challenging to reconstruct because of their 
                translucent appearance and complex light–scattering properties. Most existing 
                approaches rely on controlled lab setups or expensive satellite data, which limits 
                real-world applicability.  
                <br><br>
                We propose a novel framework that integrates a <strong>stereo depth carving module</strong>, 
                a <strong>3D CNN</strong>, and an <strong>advection module</strong>. The stereo module provides 
                coarse volume boundaries by carving empty space from stereo depth. The 3D CNN then 
                predicts volumetric density fields, while the advection module leverages temporal 
                dynamics to estimate velocity fields and enforce temporal consistency.  
                <br><br>
                <strong>Contributions:</strong>  
                (1) A stereo depth carving module that enables volumetric reconstruction from sparse views.  
                (2) An advection module that models temporal evolution, improving shape and motion 
                consistency.  
                (3) Two cloud datasets: a synthetic dataset for training and a real-world dataset for 
                evaluation, demonstrating accurate recovery of cloud geometry and motion from few 
                stereo pairs.
              </div>
              <div class="phone-teasers" style="display:flex;justify-content:center;margin-top:1em;">
                <img class="phone-teasers" src="images/clouds.jpg" alt="hpp" style="border-style:none;width:300px">
              </div>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle" class="comp-teasers">
              <img src="images/gensynth.jpg" alt="GENSYNTH teaser" style="border-style:none;width:350px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Debiasing Vision-Language Datasets with Synthetic Contrast Sets</papertitle>
              <div>
                <span style="font-size:13px;">
                  Brandon Smith<sup>*</sup>, <strong>Miguel Farinha<sup>*</sup></strong>, 
                  Siobhan Mackenzie Hall, Hannah Rose Kirk, Aleksandar Shtedritski, Max Bain
                </span>
              </div>
              <div>
              <em>NeurIPS 2023 Workshop SyntheticData4ML</em>
              </div>
              <div>
                [
                  <a target="_blank" href="https://arxiv.org/abs/2305.15407">Paper</a>  &nbsp;/&nbsp;
                  <a target="_blank" href="https://github.com/oxai/debias-gensynth">Code</a>
                ]
              </div>
              <p class="summary">
                Synthetic gender-balanced contrast sets for debiasing vision-language models.
              </p>
              
              <a href="javascript:void(0)" class="toggle-overview">[Overview]</a>
              <div class="abstract" style="display:none;">
                This work aims to evaluate and improve the reliability of <strong>model bias</strong>
                measurements in vision-language models (VLMs).
                <br><br>
                We argue that measurements of <strong>model bias</strong> lack validity due to <strong>dataset bias</strong>. 
                We demonstrate this by showing the COCO Captions dataset, the most commonly used dataset for evaluating bias, 
                contains spurious correlations between background context and the gender of people in-situ. To address this issue, 
                we propose a novel dataset debiasing pipeline to augment the COCO dataset with synthetic, 
                gender-balanced contrast sets, where only the gender of the subject is edited and the
                background is fixed.  
                <br><br>
                <strong>Contribution:</strong>  
                (1) We demonstrate spurious correlations in the COCO dataset between gender
                and context, and show their problematic effects when used to measure model bias;  
                (2) We present the <strong>GENSYNTH</strong> dataset, built from a generative pipeline for synthetic image editing, and a
                filtering pipeline using KNN with real and synthetic images to control for the quality of the generated
                images;  
                (3) We benchmark CLIP models on our GENSYNTH dataset, which has no
                spurious correlation, and cast doubts on the effectiveness of debiasing methods.
              </div>
              <div class="phone-teasers" style="display:flex;justify-content:center;margin-top:1em;">
                <img class="phone-teasers" src="images/gensynth.jpg" alt="hpp" style="border-style:none;width:300px">
              </div>
            </td>
          </tr>
        </tbody></table>

        <br><br>
        Template from <a target="_blank" href="https://jonbarron.info/">Jon Barron</a>.
      </td>
    </tr>
  </tbody></table>

  <!-- Modal for expanded images -->
  <div id="imgModal" class="modal">
    <span class="close">&times;</span>
    <img class="modal-content" id="modalImg">
  </div>

  <script>
    // Toggle overview text
    document.querySelectorAll('.toggle-overview').forEach(link => {
      link.addEventListener('click', () => {
        const abstract = link.nextElementSibling;
        if (abstract.style.display === "none") {
          abstract.style.display = "block";
          link.textContent = "[Hide Overview]";
        } else {
          abstract.style.display = "none";
          link.textContent = "[Overview]";
        }
      });
    });

    // Modal logic
    const modal = document.getElementById("imgModal");
    const modalImg = document.getElementById("modalImg");
    const closeBtn = document.querySelector(".close");

    document.querySelectorAll("td img").forEach(img => {
      img.style.cursor = "pointer";
      img.addEventListener("click", () => {
        modal.style.display = "block";
        modalImg.src = img.src;
      });
    });

    closeBtn.onclick = () => { modal.style.display = "none"; }
    modal.onclick = (event) => {
      if (event.target === modal) {
        modal.style.display = "none";
      }
    }
  </script>
  
</body>
</html>